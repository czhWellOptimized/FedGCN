{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torch-geometric\n",
    "!pip install torch_spline_conv\n",
    "!pip install torch_scatter \n",
    "!pip install torch_cluster #slow\n",
    "!pip install torch_sparse #slow\n",
    "!pip install matplotlib\n",
    "!pip install ogb\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d5QApBmLhOGP"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from models import GCN\n",
    "\n",
    "import torch_geometric\n",
    "\n",
    "import torch_sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "from utils import get_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process import generate_data, load_data\n",
    "from train_func import test, train, Lhop_Block_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n0WIOFZPqz2"
   },
   "outputs": [],
   "source": [
    "def get_K_hop_neighbors(adj_matrix, index, K):\n",
    "    adj_matrix = adj_matrix + torch.eye(adj_matrix.shape[0],adj_matrix.shape[1])  #make sure the diagonal part >= 1\n",
    "    hop_neightbor_index=index\n",
    "    for i in range(K):\n",
    "        hop_neightbor_index=torch.unique(torch.nonzero(adj[hop_neightbor_index])[:,1])\n",
    "    return hop_neightbor_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "def normalize(mx):  #adj matrix\n",
    "    \n",
    "    mx = mx + torch.eye(mx.shape[0],mx.shape[1])\n",
    "    \n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return torch.tensor(mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5CMAPT6gafH"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "\n",
    "#for compare 2-10 layer performance in appendix\n",
    "#iterations = 400\n",
    "#Adam, lr = 0.01\n",
    "\n",
    "\n",
    "def centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test, num_layers):\n",
    "    \n",
    "        model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers)\n",
    "        model.reset_parameters()\n",
    "        if args_cuda:\n",
    "                #from torch_geometric.nn import DataParallel\n",
    "                #model = DataParallel(model)\n",
    "                #model= torch.nn.DataParallel(model)\n",
    "                model=model.cuda()\n",
    "                \n",
    "                #features= torch.nn.DataParallel(features)\n",
    "                \n",
    "                features = features.cuda()\n",
    "                \n",
    "                #edge_index= torch.nn.DataParallel(edge_index)\n",
    "                \n",
    "                adj = adj.cuda()\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        \n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay)\n",
    "        \n",
    "        \n",
    "        #optimizer = optim.Adam(model.parameters(),\n",
    "        #                      lr=args_lr, weight_decay=args_weight_decay)\n",
    "        # Train model\n",
    "        best_val=0\n",
    "        for t in range(args_iterations): #make to equivalent to federated\n",
    "            loss_train, acc_train=train(t, model, optimizer, features, adj, labels, idx_train)\n",
    "            # validation\n",
    "            loss_train, acc_train= test(model, features, adj, labels, idx_train) #train after backward\n",
    "            print(t,\"train\",loss_train,acc_train)\n",
    "            loss_val, acc_val= test(model, features, adj, labels, idx_val) #validation\n",
    "            print(t,\"val\",loss_val,acc_val)\n",
    "            \n",
    "            a = open(dataset_name+'_IID_'+'centralized_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations),'a+')\n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            \n",
    "        #test  \n",
    "        loss_test, acc_test= test(model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(dataset_name+'_IID_'+'centralized_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        \n",
    "        print(\"save file as\",dataset_name+'_IID_'+'centralized_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations))\n",
    "        del model\n",
    "        del features \n",
    "        del adj\n",
    "        del labels\n",
    "        del idx_train\n",
    "        del idx_val\n",
    "        del idx_test\n",
    "        \n",
    "        return loss_test, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setdiff1d(t1, t2):\n",
    "    \n",
    "    combined = torch.cat((t1, t2))\n",
    "    uniques, counts = combined.unique(return_counts=True)\n",
    "    difference = uniques[counts == 1]\n",
    "    #intersection = uniques[counts > 1]\n",
    "    return difference\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect1d(t1, t2):\n",
    "    \n",
    "    combined = torch.cat((t1, t2))\n",
    "    uniques, counts = combined.unique(return_counts=True)\n",
    "    #difference = uniques[counts == 1]\n",
    "    intersection = uniques[counts > 1]\n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BDS_federated_GCN(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent, sample_rate =0.5, L_hop=1, num_layers=2):\n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #multilayer_GCN:n*n\n",
    "        #define model\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers)\n",
    "        global_model.reset_parameters()\n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].cuda()\n",
    "                global_model=global_model.cuda()\n",
    "                features = features.cuda()\n",
    "                adj = adj.cuda()\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # Train model\n",
    "        \n",
    "        row, col, edge_attr = adj.t().coo()\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "        \n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append((labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        if args_cuda:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))).cuda(), torch.cat(split_data_indexes))\n",
    "        else:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))), torch.cat(split_data_indexes))\n",
    "        \n",
    "        iid_indexes = iid_indexes[np.random.permutation(len(iid_indexes))]\n",
    "        \n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "            \n",
    "        communicate_indexes = []\n",
    "        in_com_train_data_indexes = []\n",
    "\n",
    "        for i in range(K):\n",
    "            if args_cuda:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i]).cuda()\n",
    "            else:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i])\n",
    "                \n",
    "            split_data_indexes[i] = split_data_indexes[i].sort()[0]\n",
    "            \n",
    "            #communicate_index=get_K_hop_neighbors(adj, split_data_indexes[i], L_hop) #normalized adj\n",
    "            \n",
    "            communicate_index = torch_geometric.utils.k_hop_subgraph(split_data_indexes[i],L_hop,edge_index)[0]\n",
    "            communicate_indexes.append(communicate_index)\n",
    "            communicate_indexes[i] = communicate_indexes[i].sort()[0]\n",
    "            \n",
    "            inter = intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)   \n",
    "            in_com_train_data_indexes.append(torch.searchsorted(communicate_indexes[i], inter).clone())   #local id in block matrix\n",
    "\n",
    "            \n",
    "            \n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        for t in range(args_iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    diff = setdiff1d(split_data_indexes[i], communicate_indexes[i])\n",
    "                    sample_index = torch.cat((split_data_indexes[i], diff[torch.randperm(len(diff))[:int(len(diff) * sample_rate)]])).clone()\n",
    "\n",
    "                    sample_index = sample_index.sort()[0]\n",
    "                    \n",
    "                    inter = intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)\n",
    "                    in_sample_train_data_index = torch.searchsorted(sample_index, inter).clone()   #local id in block matrix\n",
    "\n",
    "                    if len(in_sample_train_data_index) == 0:\n",
    "                        continue\n",
    "                    try:\n",
    "                        adj[sample_index][:,sample_index]\n",
    "                    except: #adj is empty\n",
    "                        continue\n",
    "                    \n",
    "                    acc_train = FedSage_train(epoch, models[i], optimizers[i], \n",
    "                                                        features, adj, labels, sample_index, in_sample_train_data_index)\n",
    "                acc_trains.append(acc_train)\n",
    "                \n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = in_com_train_data_indexes[0].shape[0] * states[0][key]\n",
    "                count_D=in_com_train_data_indexes[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += in_com_train_data_indexes[i].shape[0] * states[i][key]\n",
    "                    count_D += in_com_train_data_indexes[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            # Testing\n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_BDS_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "\n",
    "            \n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_BDS_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        print(\"save file as\",dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_BDS_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "        \n",
    "        del global_model\n",
    "        del features \n",
    "        del adj\n",
    "        del labels\n",
    "        del idx_train\n",
    "        del idx_val\n",
    "        del idx_test\n",
    "        while(len(models)>=1):\n",
    "            del models[0]\n",
    "        \n",
    "        return loss_test, acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedSage_plus(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent, L_hop = 1, num_layers=2):\n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #multilayer_GCN:n*n\n",
    "        #define model\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers)\n",
    "        global_model.reset_parameters()\n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].to(device)\n",
    "                global_model=global_model.to(device)\n",
    "                features = features.to(device)\n",
    "                adj = adj.to(device)\n",
    "                labels = labels.to(device)\n",
    "                idx_train = idx_train.to(device)\n",
    "                idx_val = idx_val.to(device)\n",
    "                idx_test = idx_test.to(device)\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # Train model\n",
    "        \n",
    "        row, col, edge_attr = adj.t().coo()\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "        \n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append((labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        if args_cuda:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))).to(device), torch.cat(split_data_indexes))\n",
    "        else:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))), torch.cat(split_data_indexes))\n",
    "        \n",
    "        iid_indexes = iid_indexes[np.random.permutation(len(iid_indexes))]\n",
    "        \n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "            \n",
    "        communicate_indexes = []\n",
    "        in_com_train_data_indexes = []\n",
    "        for i in range(K):\n",
    "            if args_cuda:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i]).to(device)\n",
    "            else:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i])\n",
    "                \n",
    "            split_data_indexes[i] = split_data_indexes[i].sort()[0]\n",
    "            \n",
    "            #communicate_index=get_K_hop_neighbors(adj, split_data_indexes[i], L_hop) #normalized adj\n",
    "            \n",
    "            communicate_index = torch_geometric.utils.k_hop_subgraph(split_data_indexes[i],L_hop,edge_index)[0]\n",
    "                \n",
    "            communicate_indexes.append(communicate_index)\n",
    "            communicate_indexes[i] = communicate_indexes[i].sort()[0]\n",
    "            \n",
    "            inter = intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)\n",
    "\n",
    "                \n",
    "            in_com_train_data_indexes.append(torch.searchsorted(communicate_indexes[i], inter).clone())   #local id in block matrix\n",
    "        \n",
    "        features_in_clients = []\n",
    "        #assume the linear generator learnt the optimal (the average of features of neighbor nodes)\n",
    "        #gaussian noise\n",
    "        for i in range(K):\n",
    "            #orignial features of outside neighbors of nodes in client i\n",
    "            original_feature_i = features[setdiff1d(split_data_indexes[i], communicate_indexes[i])].clone()\n",
    "            \n",
    "            gaussian_feature_i = original_feature_i + torch.normal(0, 0.1, original_feature_i.shape).to(device)\n",
    "            \n",
    "            copy_feature = features.clone()\n",
    "            \n",
    "            copy_feature[setdiff1d(split_data_indexes[i], communicate_indexes[i])] = gaussian_feature_i\n",
    "            \n",
    "            features_in_clients.append(copy_feature[communicate_indexes[i]])\n",
    "            print(features_in_clients[i].shape, communicate_indexes[i].shape)\n",
    "\n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        for t in range(args_iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    if len(in_com_train_data_indexes[i]) == 0:\n",
    "                        continue\n",
    "                    try:\n",
    "                        adj[communicate_indexes[i]][:,communicate_indexes[i]]\n",
    "                    except: #adj is empty\n",
    "                        continue\n",
    "                    acc_train = FedSage_train(epoch, models[i], optimizers[i], \n",
    "                                                        features_in_clients[i], adj, labels, communicate_indexes[i], in_com_train_data_indexes[i])\n",
    "                    \n",
    "                acc_trains.append(acc_train)\n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = in_com_train_data_indexes[0].shape[0] * states[0][key]\n",
    "                count_D=in_com_train_data_indexes[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += in_com_train_data_indexes[i].shape[0] * states[i][key]\n",
    "                    count_D += in_com_train_data_indexes[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            # Testing\n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_FedSage_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "\n",
    "            \n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_FedSage_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        print(\"save file as\",dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_FedSage_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "        \n",
    "        del global_model\n",
    "        del features \n",
    "        del adj\n",
    "        del labels\n",
    "        del idx_train\n",
    "        del idx_val\n",
    "        del idx_test\n",
    "        while(len(models)>=1):\n",
    "            del models[0]\n",
    "        \n",
    "        return loss_test, acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lhop_Block_federated_GCN(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent, L_hop, num_layers):\n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #multilayer_GCN:n*n\n",
    "        #define model\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers)\n",
    "        global_model.reset_parameters()\n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].cuda()\n",
    "                global_model=global_model.cuda()\n",
    "                features = features.cuda()\n",
    "                adj = adj.cuda()\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # Train model\n",
    "        \n",
    "        row, col, edge_attr = adj.t().coo()\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "        \n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append((labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        if args_cuda:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))).cuda(), torch.cat(split_data_indexes))\n",
    "        else:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))), torch.cat(split_data_indexes))\n",
    "        iid_indexes = iid_indexes[np.random.permutation(len(iid_indexes))]\n",
    "        \n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "            \n",
    "        communicate_indexes = []\n",
    "        in_com_train_data_indexes = []\n",
    "        for i in range(K):\n",
    "            if args_cuda:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i]).cuda()\n",
    "            else:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i])\n",
    "                \n",
    "            split_data_indexes[i] = split_data_indexes[i].sort()[0]\n",
    "            \n",
    "            #communicate_index=get_K_hop_neighbors(adj, split_data_indexes[i], L_hop) #normalized adj\n",
    "            \n",
    "            communicate_index = torch_geometric.utils.k_hop_subgraph(split_data_indexes[i],L_hop,edge_index)[0]\n",
    "                \n",
    "            communicate_indexes.append(communicate_index)\n",
    "            communicate_indexes[i] = communicate_indexes[i].sort()[0]\n",
    "            \n",
    "            inter = intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)\n",
    "\n",
    "                \n",
    "            in_com_train_data_indexes.append(torch.searchsorted(communicate_indexes[i], inter).clone())   #local id in block matrix\n",
    "\n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        for t in range(args_iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    if len(in_com_train_data_indexes[i]) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        adj[communicate_indexes[i]][:,communicate_indexes[i]]\n",
    "                    except: #adj is empty\n",
    "                        continue\n",
    "                    acc_train = Lhop_Block_matrix_train(epoch, models[i], optimizers[i], \n",
    "                                                        features, adj, labels, communicate_indexes[i], in_com_train_data_indexes[i])\n",
    "                    \n",
    "                acc_trains.append(acc_train)\n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = in_com_train_data_indexes[0].shape[0] * states[0][key]\n",
    "                count_D=in_com_train_data_indexes[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += in_com_train_data_indexes[i].shape[0] * states[i][key]\n",
    "                    count_D += in_com_train_data_indexes[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            # Testing\n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_Block_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "\n",
    "            \n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_Block_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        print(\"save file as\",dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_Block_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "        \n",
    "        del global_model\n",
    "        del features \n",
    "        del adj\n",
    "        del labels\n",
    "        del idx_train\n",
    "        del idx_val\n",
    "        del idx_test\n",
    "        while(len(models)>=1):\n",
    "            del models[0]\n",
    "        \n",
    "        return loss_test, acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ubM8c3SqXwA3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1823307/1302843733.py:107: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(140, 1433)\n",
      "test_idx_range:[1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721\n",
      " 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735\n",
      " 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749\n",
      " 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763\n",
      " 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777\n",
      " 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791\n",
      " 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805\n",
      " 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819\n",
      " 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833\n",
      " 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847\n",
      " 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861\n",
      " 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875\n",
      " 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889\n",
      " 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903\n",
      " 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917\n",
      " 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931\n",
      " 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945\n",
      " 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959\n",
      " 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973\n",
      " 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987\n",
      " 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001\n",
      " 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\n",
      " 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029\n",
      " 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043\n",
      " 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057\n",
      " 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071\n",
      " 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085\n",
      " 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099\n",
      " 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113\n",
      " 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127\n",
      " 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141\n",
      " 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155\n",
      " 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169\n",
      " 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183\n",
      " 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197\n",
      " 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211\n",
      " 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225\n",
      " 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239\n",
      " 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253\n",
      " 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267\n",
      " 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281\n",
      " 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295\n",
      " 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309\n",
      " 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323\n",
      " 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337\n",
      " 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351\n",
      " 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365\n",
      " 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379\n",
      " 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393\n",
      " 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407\n",
      " 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421\n",
      " 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435\n",
      " 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449\n",
      " 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463\n",
      " 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477\n",
      " 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491\n",
      " 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505\n",
      " 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519\n",
      " 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533\n",
      " 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547\n",
      " 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561\n",
      " 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575\n",
      " 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589\n",
      " 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603\n",
      " 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617\n",
      " 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631\n",
      " 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645\n",
      " 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659\n",
      " 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673\n",
      " 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687\n",
      " 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701\n",
      " 2702 2703 2704 2705 2706 2707]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "#from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "import torch_sparse\n",
    "\n",
    "def generate_data(number_of_nodes, class_num, link_inclass_prob, link_outclass_prob): # 模拟用的\n",
    "    \n",
    "    \n",
    "    adj=torch.zeros(number_of_nodes,number_of_nodes) #n*n adj matrix\n",
    "\n",
    "    labels=torch.randint(0,class_num,(number_of_nodes,)) #assign random label with equal probability\n",
    "    labels=labels.to(dtype=torch.long)\n",
    "    #label_node, speed up the generation of edges\n",
    "    label_node_dict=dict()\n",
    "\n",
    "    for j in range(class_num):\n",
    "            label_node_dict[j]=[]\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        label_node_dict[int(labels[i])]+=[int(i)]\n",
    "    # label_id -> [client_id ... ....]\n",
    "\n",
    "    #generate graph\n",
    "    for node_id in range(number_of_nodes):\n",
    "                j=labels[node_id]\n",
    "                for l in label_node_dict:\n",
    "                    if l==j: # 属于一个class\n",
    "                        for z in label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
    "                            if z>node_id and random.random()<link_inclass_prob:\n",
    "                                adj[node_id,z]= 1\n",
    "                                adj[z,node_id]= 1\n",
    "                    else:    # 属于不同class\n",
    "                        for z in label_node_dict[l]:\n",
    "                            if z>node_id and random.random()<link_outclass_prob:\n",
    "                                adj[node_id,z]= 1\n",
    "                                adj[z,node_id]= 1\n",
    "                              \n",
    "    adj=torch_geometric.utils.dense_to_sparse(torch.tensor(adj))[0]\n",
    "\n",
    "    #generate feature use eye matrix\n",
    "    features=torch.eye(number_of_nodes,number_of_nodes)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #seprate train,val,test\n",
    "    idx_train = torch.LongTensor(range(number_of_nodes//5))\n",
    "    idx_val = torch.LongTensor(range(number_of_nodes//5, number_of_nodes//2))\n",
    "    idx_test = torch.LongTensor(range(number_of_nodes//2, number_of_nodes))\n",
    "\n",
    "\n",
    "\n",
    "    return features.float(), adj, labels, idx_train, idx_val, idx_test\n",
    "    \n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "  \n",
    "    \n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    if dataset_str in ['cora', 'citeseer', 'pubmed']:\n",
    "        names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "        objects = []\n",
    "        for i in range(len(names)):\n",
    "            with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "                if sys.version_info > (3, 0):\n",
    "                    objects.append(pkl.load(f, encoding='latin1'))\n",
    "                else:\n",
    "                    objects.append(pkl.load(f))\n",
    "\n",
    "        x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "        print(type(x))\n",
    "        print(x.shape)\n",
    "        test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "        # print(f\"test_idx_reorder:{test_idx_reorder}\") # 应该那些需要预测的文章id\n",
    "        test_idx_range = np.sort(test_idx_reorder)\n",
    "        print(f\"test_idx_range:{test_idx_range}\") # [1708，2707]，预测1000篇文章 \n",
    "        \n",
    "        if dataset_str == 'citeseer':\n",
    "            # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "            # Find isolated nodes, add them as zero-vecs into the right position\n",
    "            test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "            tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "            tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "            tx = tx_extended\n",
    "            ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "            ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "            ty = ty_extended\n",
    "\n",
    "        features = sp.vstack((allx, tx)).tolil() # 按行堆叠，其他维度需要相同\n",
    "        features[test_idx_reorder, :] = features[test_idx_range, :] # \n",
    "        adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "        labels = np.vstack((ally, ty)) # y\n",
    "        labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "        number_of_nodes=adj.shape[0]\n",
    "\n",
    "\n",
    "        idx_test = test_idx_range.tolist()\n",
    "        idx_train = range(len(y))\n",
    "        idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "        idx_train = torch.LongTensor(idx_train)\n",
    "        idx_val = torch.LongTensor(idx_val)\n",
    "        idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "        #features = normalize(features) #cannot converge if use SGD, why??????????\n",
    "        #adj = normalize(adj)    # no normalize adj here, normalize it in the training process\n",
    "\n",
    "\n",
    "        features=torch.tensor(features.toarray()).float()\n",
    "        adj = torch.tensor(adj.toarray())\n",
    "        adj = torch_sparse.tensor.SparseTensor.from_edge_index(torch_geometric.utils.dense_to_sparse(adj)[0])\n",
    "        #edge_index=torch_geometric.utils.dense_to_sparse(torch.tensor(adj.toarray()))[0]\n",
    "        labels=torch.tensor(labels)\n",
    "        labels=torch.argmax(labels,dim=1)\n",
    "    elif dataset_str in ['ogbn-arxiv', 'ogbn-products', 'ogbn-mag', 'ogbn-papers100M']: #'ogbn-mag' is heteregeneous\n",
    "        #from ogb.nodeproppred import NodePropPredDataset\n",
    "        from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "        # Download and process data at './dataset/.'\n",
    "\n",
    "        #dataset = NodePropPredDataset(name = dataset_str, root = 'dataset/')\n",
    "        dataset = PygNodePropPredDataset(name='ogbn-arxiv',\n",
    "                                     transform=torch_geometric.transforms.ToSparseTensor())\n",
    "\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        idx_train, idx_val, idx_test = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "        idx_train = torch.LongTensor(idx_train)\n",
    "        idx_val = torch.LongTensor(idx_val)\n",
    "        idx_test = torch.LongTensor(idx_test)\n",
    "        data = dataset[0]\n",
    "        \n",
    "        features = data.x #torch.tensor(graph[0]['node_feat'])\n",
    "        labels = data.y.reshape(-1) #torch.tensor(graph[1].reshape(-1))\n",
    "        adj = data.adj_t.to_symmetric()\n",
    "        #edge_index = torch.tensor(graph[0]['edge_index'])\n",
    "        #adj = torch_geometric.utils.to_dense_adj(torch.tensor(graph[0]['edge_index']))[0]\n",
    "\n",
    "    return features.float(), adj, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "#'cora', 'citeseer', 'pubmed' #simulate #other dataset twitter, \n",
    "dataset_name=\"cora\"#'ogbn-arxiv'\n",
    "\n",
    "if dataset_name == 'simulate':\n",
    "    number_of_nodes=200\n",
    "    class_num=3\n",
    "    link_inclass_prob=10/number_of_nodes  #when calculation , remove the link in itself\n",
    "    #EGCN good when network is dense 20/number_of_nodes  #fails when network is sparse. 20/number_of_nodes/5\n",
    "\n",
    "    link_outclass_prob=link_inclass_prob/20\n",
    "\n",
    "\n",
    "    features, adj, labels, idx_train, idx_val, idx_test = generate_data(number_of_nodes,  class_num, link_inclass_prob, link_outclass_prob)               \n",
    "else:\n",
    "    features, adj, labels, idx_train, idx_val, idx_test = load_data(dataset_name)\n",
    "    class_num = labels.max().item() + 1\n",
    "\n",
    "# print(type(features))\n",
    "# print(features.shape)\n",
    "indices = torch.nonzero(features)\n",
    "row = indices[:,0]\n",
    "col = indices[:,1]\n",
    "\n",
    "# print(f\"row:{row}\")\n",
    "# print(f\"col:{col}\")\n",
    "# print(features) # 每篇文章出现了哪些单词\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name in ['simulate', 'cora', 'citeseer', 'pubmed']:\n",
    "    args_hidden = 16\n",
    "else:\n",
    "    args_hidden = 256\n",
    "\n",
    "args_dropout = 0.5\n",
    "args_lr = 1.0\n",
    "args_weight_decay = 5e-4     #L2 penalty\n",
    "args_epochs = 3\n",
    "args_no_cuda = False\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "args_device_num = class_num #split data into args_device_num parts\n",
    "args_iterations = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for testing\n",
    "centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test, num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test, num_layers = 2)\n",
    "    \n",
    "for args_epochs in [3]:\n",
    "    for args_random_assign in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        for i in range(10):\n",
    "            Lhop_Block_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 0, num_layers = 2)\n",
    "            BDS_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign)\n",
    "            Lhop_Block_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 1, num_layers = 2)\n",
    "            Lhop_Block_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 2, num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_l in range(1, 11):\n",
    "    for i in range(10):#10 times\n",
    "        centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test, num_layers = num_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for args_epochs in [3]:\n",
    "    for args_random_assign in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        for num_l in range(2, 11):\n",
    "            for i in range(10):\n",
    "                Block_federated_multilayer_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, num_layers = num_l)\n",
    "                Lhop_Block_federated_multilayer_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 1, num_layers = num_l)\n",
    "                Lhop_Block_federated_multilayer_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 2, num_layers = num_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
